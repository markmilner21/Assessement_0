gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index, data = selected_columns, method = "lm",trControl=trainControl(method = "cv",number=5))
## Definition of an interaction model we will refer to as **int**:
modelcvint <- train(new_deaths_smoothed_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index^2, data = selected_columns, method = "lm",trControl=trainControl(method = "cv",number=5))
comparison=c(testlin=R2(pred = predict(modelcv,d2test),obs = selected_columns$new_deaths_smoothed_per_million),
trainlin=R2(pred = predict(modelcv,d2train),obs = selected_columns$new_deaths_smoothed_per_million),
testint=R2(pred = predict(modelcvint,d2test),obs = selected_columns$new_deaths_smoothed_per_million),
trainint=R2(pred = predict(modelcvint,d2train),obs = selected_columns$new_deaths_smoothed_per_million)
)
modelcv <- train(new_deaths_smoothed_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index, data = selected_columns, method = "lm",trControl=trainControl(method = "cv",number=5))
## Definition of an interaction model we will refer to as **int**:
modelcvint <- train(new_deaths_smoothed_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index^2, data = selected_columns, method = "lm",trControl=trainControl(method = "cv",number=5))
comparison=c(testlin=R2(pred = predict(modelcv,selected_columns),obs = selected_columns$new_deaths_smoothed_per_million),
trainlin=R2(pred = predict(modelcv,selected_columns),obs = selected_columns$new_deaths_smoothed_per_million),
testint=R2(pred = predict(modelcvint,selected_columns),obs = selected_columns$new_deaths_smoothed_per_million),
trainint=R2(pred = predict(modelcvint,selected_columns),obs = selected_columns$new_deaths_smoothed_per_million)
)
print(comparison)
dlm2=lm(price~carat + depth + table +x+y+z,data=d2)
if(!require("ggplot2")) install.packages("ggplot2")
if(!require("caret")) install.packages("caret")
library("ggplot2") # Ironically we're using this for the data, and not for the plotting!
head(diamonds)
d2=data.frame(diamonds)
numericcols=c(1,5:10) # Refers to columns containing numerical data (index 1 and 5 to 10)
catcols=2:4 # refers to columns containg categorical data (index 2 to 4)
## Perform a minimal data sanitization step
d2=d2[-which((d2$x==0)|(d2$y==0)|(d2$z==0)|(d2$y>15)|(d2$z>15)),]
# removes rows from the dataset where
# 1. x, y or z is zero (doesn't make sense) or y or z > 15 because they are physcially unrealistic
plot(d2[,numericcols],pch=19,col="#00000033",cex=0.7)
# create a scatter plot matrix for numerical columns
# the plot function creates a scatter plot matrix for all the numeric columns
# pch=19 =>sets plot character to a solid circle i.e. the points
# col makes the points a certain colour
# cex reduced the size to 70% of default
# The resulting plot will be a grid of scatter plots, comparing each pair of numeric variables from the diamonds dataset after the sanitization step.
#                 Monthly          Annual          Five-year        Ten-year        Twenty-year
# Year, Month,  Anomaly, Unc.,   Anomaly, Unc.,   Anomaly, Unc.,   Anomaly, Unc.,   Anomaly, Unc.ÃŸ
temperature=read.table("https://berkeley-earth-temperature.s3.us-west-1.amazonaws.com/Global/Land_and_Ocean_complete.txt",skip = 86,nrows = 1997)
#skips first 86 lines
colnames(temperature)=c("Year","Month","MA","MACI","AA","AACI","A5","A5CI","A10","A10CI","A20","A20CI")
# ci is confidence interval
temperature$Time=temperature$Year+(temperature$Month-1)/12
# adding new column time e.f. for feb (month 2), time would be year + 1/12 i.e. one month into the year
plot(temperature$Time,temperature$MA,xlab="Date",ylab="Monthly Anomaly")
# plot created with time and monthly anaomoly
dlm2=lm(price~carat + depth + table +x+y+z,data=d2)
# lm fits a linear model. formula explains that price is a response variable and the others are all predictors
summary(dlm2)
# Provides a detailed summary of the linear model, including information about coefficients, statistical significance, and the model's goodness of fit.
set.seed(2)
## Downsample the data for computational convenience
mysamples=sample(dim(d2)[1],2000)
# retrieves number of rows in dataset d2 and randomely samples 2000 - this just selects the indieces
smalld2=d2[mysamples,]
# this gets the rows that correspond to these indices
## Make a test/train split
s=createDataPartition(1:dim(smalld2)[1],p=0.8,list=FALSE) # 80% training 20% test
d2train=smalld2[s,]
d2test=smalld2[-s,]
## Learn a model on the training data, and use it to predict the test data
## Definition of a linear model we will refer to as **lin**:
modelcv <- train(price ~ ., data = d2train, method = "lm",trControl=trainControl(method = "cv",number=5))
## Definition of an interaction model we will refer to as **int**:
modelcvint <- train(price ~ .^2, data = d2train, method = "lm",trControl=trainControl(method = "cv",number=5))
comparison=c(testlin=R2(pred = predict(modelcv,d2test),obs = d2test$price),
trainlin=R2(pred = predict(modelcv,d2train),obs = d2train$price),
testint=R2(pred = predict(modelcvint,d2test),obs = d2test$price),
trainint=R2(pred = predict(modelcvint,d2train),obs = d2train$price)
)
print(comparison)
set.seed(1)
## Downsample the data for computational convenience
## Make a test/train split
temperature2=na.omit(temperature[,c("Time","MA")])
temps=createDataPartition(1:dim(temperature2)[1],p=0.8,list=FALSE)
temptrain=temperature2[temps,]
temptest=temperature2[-temps,]
## Learn a model on the training data, and use it to predict the test data
tempmodelcv <- train(MA ~ Time, data = temptrain, method = "lm",trControl=trainControl(method = "cv",number=5))
tempmodelcvint <- train(MA ~ poly(Time,5), data = temptrain, method = "lm",trControl=trainControl(method = "cv",number=5))
head(JHU_c)
us_data <- owid %>%
filter(location == "United States")
us_data <- owid %>%
filter(location == "United States")
columns(us_data)
us_data <- owid %>%
filter(location == "United States")
list(us_data)
us_data <- owid %>%
filter(location == "United States")
colnames(us_data)
# Load ggplot2
library(ggplot2)
# Create the plot - this plot has too much data and is weird / not very representative of anything
ggplot(us_data, aes(x = date, y = total_deaths)) +
geom_point(color = "blue") +  # Add points
labs(x = "Date", y = "Total Deaths", title = "Total Death Count Over Time") +
theme_minimal()
# Install ggplot2 if you haven't already
# install.packages("ggplot2")
# Load ggplot2
library(ggplot2)
# Plot using ggplot2
ggplot(selected_data, aes(x = date, y = total_deaths, group=location, color=location)) +
geom_line() +
labs(title = "Total Deaths over Time", x = "Date", y = "Total Deaths") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
ggplot(selected_data, aes(x = date, y = total_deaths, group=location, color=location)) +
geom_line() +
labs(title = "Total Deaths over Time", x = "Date", y = "Total Deaths") +
theme_minimal()
# Split data into training and testing sets (80% training, 20% testing)
set.seed(123)  # For reproducibility
train_index <- createDataPartition(selected_columns$new_deaths_smoothed_per_million, p = 0.8, list = FALSE)
train_data <- selected_columns[train_index, ]
test_data <- selected_columns[-train_index, ]
library(rpart)
# Fit a decision tree model
tree_model <- rpart(new_deaths_smoothed_per_million ~ ., data = train_data)
# Make predictions
tree_predictions <- predict(tree_model, newdata = test_data)
# Calculate Mean Absolute Error
mae_tree <- mean(abs(test_data$new_deaths_smoothed_per_million - tree_predictions))
print(paste("Mean Absolute Error (Decision Tree):", mae_tree))
# Calculate R-squared
rsq_tree <- cor(test_data$new_deaths_smoothed_per_million, tree_predictions)^2
print(paste("R-squared (Decision Tree):", rsq_tree))
# Load necessary libraries
library(dplyr)
library(tidyr)
# Load your datasets
# Assuming you have already loaded your datasets (e.g., owid, hdi, etc.)
# For example: owid <- read.csv("path_to_owid_dataset.csv")
# hdi <- read.csv("path_to_hdi_dataset.csv")
# Select relevant columns from the OWID dataset
selected_columns <- owid %>%
select(location, new_deaths_smoothed_per_million, population_density, median_age,
aged_65_older, aged_70_older, gdp_per_capita, extreme_poverty,
cardiovasc_death_rate, handwashing_facilities, life_expectancy,
human_development_index)
# Remove rows with missing total_deaths_per_million
selected_columns <- selected_columns %>%
filter(!is.na(new_deaths_smoothed_per_million))
print(head(selected_columns))
# Create the linear regression model
linear_model <- lm(new_deaths_smoothed_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index,
data = selected_columns)  # Use final_data for the dataset
# lm fits a linear model. formula explains that price is a response variable and the others are all predictors
summary(linear_model)
# Provides a detailed summary of the linear model, including information about coefficients, statistical significance, and the model's goodness of fit.
# Load necessary libraries
library(dplyr)
library(tidyr)
# Load your datasets
# Assuming you have already loaded your datasets (e.g., owid, hdi, etc.)
# For example: owid <- read.csv("path_to_owid_dataset.csv")
# hdi <- read.csv("path_to_hdi_dataset.csv")
# Select relevant columns from the OWID dataset
selected_columns <- us_data %>%
select(location, new_deaths_smoothed_per_million, population_density, median_age,
aged_65_older, aged_70_older, gdp_per_capita, extreme_poverty,
cardiovasc_death_rate, handwashing_facilities, life_expectancy,
human_development_index)
# Remove rows with missing total_deaths_per_million
selected_columns <- selected_columns %>%
filter(!is.na(new_deaths_smoothed_per_million))
print(head(selected_columns))
# Create the linear regression model
linear_model <- lm(new_deaths_smoothed_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index,
data = selected_columns)  # Use final_data for the dataset
# Create the linear regression model
linear_model <- lm(new_deaths_smoothed_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index,
data = us_data)  # Use final_data for the dataset
# Load necessary libraries
library(dplyr)
library(tidyr)
# Load your datasets
# Assuming you have already loaded your datasets (e.g., owid, hdi, etc.)
# For example: owid <- read.csv("path_to_owid_dataset.csv")
# hdi <- read.csv("path_to_hdi_dataset.csv")
# Select relevant columns from the OWID dataset
selected_columns <- owid %>%
select(location, new_deaths_smoothed_per_million, population_density, median_age,
aged_65_older, aged_70_older, gdp_per_capita, extreme_poverty,
cardiovasc_death_rate, handwashing_facilities, life_expectancy,
human_development_index)
# Remove rows with missing total_deaths_per_million
selected_columns <- selected_columns %>%
filter(!is.na(new_deaths_smoothed_per_million))
print(head(selected_columns))
# Create the linear regression model
linear_model <- lm(new_deaths_smoothed_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index,
data = selected_columns)  # Use final_data for the dataset
# lm fits a linear model. formula explains that price is a response variable and the others are all predictors
summary(linear_model)
# Provides a detailed summary of the linear model, including information about coefficients, statistical significance, and the model's goodness of fit.
# Load necessary libraries
library(dplyr)
library(tidyr)
# Load your datasets
# Assuming you have already loaded your datasets (e.g., owid, hdi, etc.)
# For example: owid <- read.csv("path_to_owid_dataset.csv")
# hdi <- read.csv("path_to_hdi_dataset.csv")
# Select relevant columns from the OWID dataset
selected_columns <- owid %>%
select(location, total_deaths, population_density, median_age,
aged_65_older, aged_70_older, gdp_per_capita, extreme_poverty,
cardiovasc_death_rate, handwashing_facilities, life_expectancy,
human_development_index)
# Remove rows with missing total_deaths_per_million
selected_columns <- selected_columns %>%
filter(!is.na(new_deaths_smoothed_per_million))
# Load necessary libraries
library(dplyr)
library(tidyr)
# Load your datasets
# Assuming you have already loaded your datasets (e.g., owid, hdi, etc.)
# For example: owid <- read.csv("path_to_owid_dataset.csv")
# hdi <- read.csv("path_to_hdi_dataset.csv")
# Select relevant columns from the OWID dataset
selected_columns <- owid %>%
select(location, total_deaths, population_density, median_age,
aged_65_older, aged_70_older, gdp_per_capita, extreme_poverty,
cardiovasc_death_rate, handwashing_facilities, life_expectancy,
human_development_index)
# Remove rows with missing total_deaths_per_million
selected_columns <- selected_columns %>%
filter(!is.na(total_deaths))
print(head(selected_columns))
# Create the linear regression model
linear_model <- lm(total_deaths ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index,
data = selected_columns)  # Use final_data for the dataset
# lm fits a linear model. formula explains that price is a response variable and the others are all predictors
summary(linear_model)
# Provides a detailed summary of the linear model, including information about coefficients, statistical significance, and the model's goodness of fit.
# Create the linear regression model
linear_model <- lm(total_deaths_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index,
data = selected_columns)  # Use final_data for the dataset
# Load necessary libraries
library(dplyr)
library(tidyr)
# Load your datasets
# Assuming you have already loaded your datasets (e.g., owid, hdi, etc.)
# For example: owid <- read.csv("path_to_owid_dataset.csv")
# hdi <- read.csv("path_to_hdi_dataset.csv")
# Select relevant columns from the OWID dataset
selected_columns <- owid %>%
select(location, total_deaths_per_million, population_density, median_age,
aged_65_older, aged_70_older, gdp_per_capita, extreme_poverty,
cardiovasc_death_rate, handwashing_facilities, life_expectancy,
human_development_index)
# Remove rows with missing total_deaths_per_million
selected_columns <- selected_columns %>%
filter(!is.na(total_deaths_per_million))
print(head(selected_columns))
# Create the linear regression model
linear_model <- lm(total_deaths_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index,
data = selected_columns)  # Use final_data for the dataset
# lm fits a linear model. formula explains that price is a response variable and the others are all predictors
summary(linear_model)
# Provides a detailed summary of the linear model, including information about coefficients, statistical significance, and the model's goodness of fit.
# Create the linear regression model
linear_model <- lm(total_deaths_per_million ~ population_density +
median_age +
aged_65_older +
aged_70_older +
gdp_per_capita +
extreme_poverty +
cardiovasc_death_rate +
handwashing_facilities +
life_expectancy +
human_development_index,
data = us_data)  # Use final_data for the dataset
ggplot(selected_data, aes(x = date, y = total_deaths_per_million, group=location, color=location)) +
geom_line() +
labs(title = "Total Deaths (per million) over Time", x = "Date", y = "Total Deaths") +
theme_minimal()
ggplot(selected_data, aes(x = as.Date(date), y = total_deaths_per_million, group=location, color=location)) +
geom_line() +
labs(title = "Total Deaths (per million) over Time", x = "Date", y = "Total Deaths") +
scale_x_date(date_breaks = "12 months", date_labels = "%b %Y") +
theme_minimal()
library(ggplot2)
library(ggridges)
#ggplot(selected_data, aes(x = date, y = location, height = new_cases_smoothed, fill = location, color=location)) +
# geom_density_ridges(stat = "identity") +
#labs(title = "COVID-19 New Cases (Smoothed) by Country",
#    x = "Date",
#   y = "Country") +
#  theme_minimal() +
theme(legend.position = "none")
selected_data <- selected_data[!is.na(selected_data$new_cases_smoothed), ]  # Remove rows with NA
# Step 3: Create the ridgeline plot
ggplot(selected_data, aes(x = date, y = location, height = new_cases_smoothed, fill = location, color = location)) +
geom_density_ridges(stat = "identity", scale = 5, rel_min_height = 0.01) +
labs(title = "COVID-19 New Cases (Smoothed) by Country",
x = "Date",
y = "Country") +
theme_minimal() +
theme(legend.position = "none")
library(ggplot2)
# Plot using ggplot2
ggplot(selected_data, aes(x = as.Date(date), y = total_deaths_per_million, group=location, color=location)) +
geom_line() +
labs(title = "Total Deaths (per million) over Time", x = "Date", y = "Total Deaths") +
scale_x_date(date_breaks = "12 months", date_labels = "%b %Y") +
theme_minimal()
library(ggplot2)
library(ggridges)
#ggplot(selected_data, aes(x = date, y = location, height = new_cases_smoothed, fill = location, color=location)) +
# geom_density_ridges(stat = "identity") +
#labs(title = "COVID-19 New Cases (Smoothed) by Country",
#    x = "Date",
#   y = "Country") +
#  theme_minimal() +
theme(legend.position = "none")
selected_data <- selected_data[!is.na(selected_data$new_cases_smoothed), ]  # Remove rows with NA
# Step 3: Create the ridgeline plot
ggplot(selected_data, aes(x = date, y = location, height = new_cases_smoothed, fill = location, color = location)) +
geom_density_ridges(stat = "identity", scale = 5, rel_min_height = 0.01) +
labs(title = "COVID-19 New Cases (Smoothed) by Country",
x = "Date",
y = "Country") +
theme_minimal() +
theme(legend.position = "none")
library(ggplot2)
library(ggridges)
selected_data <- selected_data[!is.na(selected_data$new_cases_smoothed), ]  # Remove rows with NA
# Step 3: Create the ridgeline plot
ggplot(selected_data, aes(x = date, y = location, height = new_cases_smoothed, fill = location, color = location)) +
geom_density_ridges(stat = "identity", scale = 5, rel_min_height = 0.01) +
labs(title = "COVID-19 New Cases (Smoothed) by Country",
x = "Date",
y = "Country") +
theme_minimal() +
theme(legend.position = "none")
library(ggplot2)
library(ggridges)
selected_data <- selected_data[!is.na(selected_data$new_cases_smoothed), ]  # Remove rows with NA
# Step 3: Create the ridgeline plot
ggplot(selected_data, aes(x = date, y = location, height = new_cases_smoothed, group=location, fill = location, color = location)) +
geom_density_ridges(stat = "identity", scale = 5, rel_min_height = 0.01) +
labs(title = "COVID-19 New Cases (Smoothed) by Country",
x = "Date",
y = "Country") +
theme_minimal() +
theme(legend.position = "none")
library(ggplot2)
library(ggridges)
selected_data <- selected_data[!is.na(selected_data$new_cases_smoothed), ]  # Remove rows with NA
# Step 3: Create the ridgeline plot
ggplot(selected_data, aes(x = date, y = location, height = new_cases_smoothed, group=location, fill = location, color = location)) +
geom_density_ridges(stat = "identity", scale = 5, rel_min_height = 0.01) +
labs(title = "COVID-19 New Cases (Smoothed) by Country",
x = "Date",
y = "Country") +
theme_minimal() +
theme(legend.position = "none")
library(ggplot2)
library(ggridges)
selected_data <- selected_data[!is.na(selected_data$new_cases_smoothed), ]  # Remove rows with NA
# Step 3: Create the ridgeline plot
ggplot(selected_data, aes(x = date, y = location, height = new_cases_smoothed, fill = location, color = location)) +
geom_density_ridges(stat = "identity", scale = 5, rel_min_height = 0.01) +
labs(title = "COVID-19 New Cases (Smoothed) by Country",
x = "Date",
y = "Country") +
theme_minimal() +
theme(legend.position = "none")
library(ggplot2)
library(ggridges)
selected_data <- selected_data[!is.na(selected_data$new_cases_smoothed), ]  # Remove rows with NA
# Step 3: Create the ridgeline plot
ggplot(selected_data, aes(x = date, y = location, height = new_cases_smoothed, group=location, fill = location, color = location)) +
geom_density_ridges(stat = "identity", scale = 5, rel_min_height = 0.01) +
labs(title = "COVID-19 New Cases (Smoothed) by Country",
x = "Date",
y = "Country") +
theme_minimal() +
theme(legend.position = "none")
#https://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html#6.%20Change
cases = ggplot(selected_data, aes(x = as.Date(date), y = new_cases_smoothed, color = location, group=location)) +
geom_line() +
facet_wrap(~ location, scales = "free_y") +  # Create small multiples by location
labs(title = "Smoothed New COVID-19 Cases Over Time by Country",
x = "Date",
y = "New Cases (Smoothed)") +
theme_minimal() +
theme(legend.position = "none")  # Hide legend if not needed
cases
stacked_data <- selected_data %>%
group_by(date, location) %>%
summarise(total_cases = sum(new_cases_smoothed, na.rm = TRUE), .groups = 'drop')
# Create stacked area plot
stacked_area_plot <- ggplot(stacked_data, aes(x = as.Date(date), y = total_cases, fill = location)) +
geom_area(alpha = 0.5) +  # Adjust alpha for transparency
labs(title = "Stacked Area Chart of COVID-19 New Cases Over Time",
x = "Date",
y = "Total New Cases") +
theme_minimal() +
theme(legend.position = "right")
print(stacked_area_plot)
stacked_data <- selected_data %>%
group_by(date, location) %>%
summarise(total_cases = sum(new_cases_smoothed, na.rm = TRUE), .groups = 'drop')
# Create stacked area plot
stacked_area_plot <- ggplot(stacked_data, aes(x = as.Date(date), y = total_cases, fill = location)) +
geom_area(alpha = 0.5) +  # Adjust alpha for transparency
labs(title = "Stacked Area Chart of COVID-19 New Cases Over Time",
x = "Date",
y = "Total New Cases") +
theme_minimal() +
theme(legend.position = "right")
print(stacked_area_plot)
